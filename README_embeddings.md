# Embeddings Iterator Documentation

This directory contains several utilities for working with the embeddings generated by `make_embeddings_openai.py`.

## Files Overview

### 1. `iterate_embeddings.py`
**Main class-based iterator for comprehensive embedding operations**

Features:
- Load and iterate through embeddings from `embeddings.pkl`
- Find similar documents using cosine similarity
- Export subsets of embeddings
- Get statistical information about embeddings
- Batch processing capabilities
- Integration with original CSV metadata

```python
from iterate_embeddings import EmbeddingsIterator

# Initialize
iterator = EmbeddingsIterator()

# Get basic info
info = iterator.get_embedding_info()

# Find similar documents
similar = iterator.find_similar_documents("DOCUMENT_ID", top_k=5)

# Iterate through embeddings
for doc_id, embedding in iterator.iterate_embeddings():
    print(f"Document: {doc_id}, Embedding shape: {len(embedding)}")
```

### 2. `embedding_utils.py`
**Simple command-line utility for quick exploration**

Can be used both as a command-line tool and interactively:

```bash
# Command line usage
python embedding_utils.py stats                    # Show statistics
python embedding_utils.py list                     # List all document IDs
python embedding_utils.py show DOCUMENT_ID         # Show specific embedding
python embedding_utils.py search PATTERN           # Search IDs by pattern
python embedding_utils.py export output.csv        # Export to CSV

# Interactive mode
python embedding_utils.py
> stats
> show ABORIGINAL_ID_POP_PROJ
> search aboriginal
> quit
```

### 3. `embedding_examples.py`
**Comprehensive examples showing various use cases**

Demonstrates:
- Similarity search
- Batch processing
- Filtering by agency
- Embedding analysis (finding central/outlier documents)
- Keyword search in metadata

```bash
python embedding_examples.py
```

## Key Classes and Methods

### EmbeddingsIterator Class

#### Initialization
```python
iterator = EmbeddingsIterator(
    embeddings_path="embeddings.pkl",  # Path to embeddings file
    csv_path="datasets.csv"            # Path to original CSV data
)
```

#### Core Methods

**`get_embedding_info()`** - Get basic embedding statistics
- Returns: Dict with total count, dimensions, sample IDs

**`iterate_embeddings(batch_size=None)`** - Iterate through embeddings
- batch_size=None: Iterate individually (doc_id, embedding)
- batch_size=N: Iterate in batches of N items

**`find_similar_documents(doc_id, top_k=5)`** - Find similar documents
- Returns: List of (doc_id, similarity_score) tuples

**`get_document_metadata(doc_id)`** - Get original CSV metadata
- Returns: Dict with title, description, agency, tags, etc.

**`search_by_text_embedding(query_embedding, top_k=10)`** - Search with custom embedding
- Useful for semantic search with new queries

**`export_embeddings_subset(doc_ids, output_path)`** - Export subset to new pickle file

**`get_embeddings_matrix()`** - Convert to numpy matrix format
- Returns: (embeddings_matrix, list_of_ids)

**`get_statistics()`** - Get statistical information
- Returns: Dict with mean, std, min, max values

## Common Use Cases

### 1. Finding Similar Documents
```python
iterator = EmbeddingsIterator()
similar_docs = iterator.find_similar_documents("ABORIGINAL_ID_POP_PROJ", top_k=10)

for doc_id, similarity in similar_docs:
    metadata = iterator.get_document_metadata(doc_id)
    print(f"{similarity:.4f} - {metadata['title']}")
```

### 2. Batch Processing
```python
for batch in iterator.iterate_embeddings(batch_size=100):
    # Process 100 embeddings at a time
    embeddings_array = np.array([emb for _, emb in batch])
    # Perform batch operations...
```

### 3. Filtering and Exporting
```python
# Filter by agency
abs_docs = [row['id'] for _, row in iterator.df.iterrows() 
           if row['agency'] == 'ABS']

# Export subset
iterator.export_embeddings_subset(abs_docs, "abs_only.pkl")
```

### 4. Statistical Analysis
```python
embeddings_matrix, ids = iterator.get_embeddings_matrix()

# Find document closest to centroid
centroid = embeddings_matrix.mean(axis=0)
distances = np.linalg.norm(embeddings_matrix - centroid, axis=1)
most_central_idx = np.argmin(distances)
most_central_doc = ids[most_central_idx]
```

### 5. Semantic Search
```python
# If you have a query embedding from OpenAI
query_embedding = [0.1, -0.2, 0.3, ...]  # Your query vector
results = iterator.search_by_text_embedding(query_embedding, top_k=10)
```

## Requirements

The scripts require the following Python packages:
- `numpy` - For numerical operations
- `pandas` - For CSV data handling
- `scikit-learn` - For cosine similarity calculations
- `pickle` - For loading embeddings (built-in)

Install with:
```bash
pip install numpy pandas scikit-learn
```

## File Structure

```
embeddings.pkl          # Generated by make_embeddings_openai.py
datasets.csv             # Original dataset metadata
iterate_embeddings.py    # Main iterator class
embedding_utils.py       # CLI utility
embedding_examples.py    # Usage examples
```

## Performance Notes

- The embeddings file contains 1,216 documents with 1,536-dimensional vectors
- Each embedding is normalized (magnitude ≈ 1.0)
- Similarity calculations use cosine similarity
- For large-scale operations, consider using the batch processing methods
- The full embeddings matrix is ~7.4MB in memory (1216 × 1536 × 4 bytes)

## Example Output

```
=== Embedding Information ===
total_embeddings: 1216
embedding_dimension: 1536
sample_ids: ['ABORIGINAL_ID_POP_PROJ', 'ABORIGINAL_ID_POP_PROJ_REMOTE', ...]
data_type: float

=== Similar Documents to ABORIGINAL_ID_POP_PROJ ===
ID: ABORIGINAL_POP_PROJ, Similarity: 0.9747
  Title: Projected population, Aboriginal and Torres Strait Islander peoples...
ID: ABORIGINAL_ID_POP_PROJ_REMOTE, Similarity: 0.9395
  Title: Projected resident population, Aboriginal and Torres Strait Islander...
```

## Integration with API

These utilities can be integrated into a larger API for:
- Document similarity search
- Semantic search endpoints
- Content recommendation
- Clustering and categorization
- Data exploration interfaces

The embeddings represent rich semantic information about each dataset, enabling powerful search and discovery capabilities.
